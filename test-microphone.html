<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Microphone Test</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      max-width: 600px;
      margin: 50px auto;
      padding: 20px;
      text-align: center;
    }
    .status {
      padding: 20px;
      margin: 20px 0;
      border-radius: 8px;
      font-weight: bold;
    }
    .success { background: #d4edda; color: #155724; }
    .error { background: #f8d7da; color: #721c24; }
    .info { background: #d1ecf1; color: #0c5460; }
    button {
      padding: 15px 30px;
      font-size: 18px;
      cursor: pointer;
      margin: 10px;
    }
    canvas {
      border: 2px solid #333;
      border-radius: 8px;
      margin: 20px 0;
    }
  </style>
</head>
<body>
  <h1>üé§ Microphone & Speech Test</h1>
  
  <div id="status" class="status info">
    Click "Test Microphone" to begin
  </div>
  
  <button id="testBtn" onclick="startTest()">Test Microphone</button>
  <button id="stopBtn" onclick="stopTest()" style="display:none;">Stop</button>
  
  <canvas id="visualizer" width="400" height="100" style="display:none;"></canvas>
  
  <div id="transcription" style="margin-top: 20px; font-style: italic; min-height: 30px;"></div>
  
  <div id="debug" style="margin-top: 30px; text-align: left; font-family: monospace; font-size: 12px; background: #f5f5f5; padding: 15px; border-radius: 8px;"></div>

  <script>
    let audioContext, analyser, microphone, recognition, animationId;
    const canvas = document.getElementById('visualizer');
    const ctx = canvas.getContext('2d');
    const statusDiv = document.getElementById('status');
    const debugDiv = document.getElementById('debug');
    const transcriptionDiv = document.getElementById('transcription');

    function log(msg) {
      console.log(msg);
      debugDiv.innerHTML += msg + '<br>';
    }

    async function startTest() {
      debugDiv.innerHTML = '';
      log('üöÄ Starting test...');
      
      try {
        // Test 1: Check browser support
        log('‚úì Browser: ' + navigator.userAgent);
        
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const AudioContext = window.AudioContext || window.webkitAudioContext;
        
        if (!SpeechRecognition) {
          throw new Error('‚ùå Web Speech API not supported. Use Chrome, Edge, or Safari.');
        }
        log('‚úì Web Speech API supported');
        
        if (!AudioContext) {
          throw new Error('‚ùå Web Audio API not supported');
        }
        log('‚úì Web Audio API supported');
        
        // Test 2: Request microphone permission
        log('üé§ Requesting microphone permission...');
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        log('‚úì Microphone access granted');
        
        // Test 3: Initialize audio context
        audioContext = new AudioContext();
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        microphone = audioContext.createMediaStreamSource(stream);
        microphone.connect(analyser);
        log('‚úì Audio context initialized');
        
        // Test 4: Initialize speech recognition
        recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.interimResults = true;
        recognition.lang = 'en-US';
        
        recognition.onstart = () => {
          log('‚úì Speech recognition started');
          statusDiv.className = 'status success';
          statusDiv.textContent = '‚úÖ Recording! Speak now and watch the visualizer...';
        };
        
        recognition.onresult = (event) => {
          let final = '';
          let interim = '';
          
          for (let i = event.resultIndex; i < event.results.length; i++) {
            if (event.results[i].isFinal) {
              final += event.results[i][0].transcript;
            } else {
              interim += event.results[i][0].transcript;
            }
          }
          
          if (final) {
            log('üìù Final: ' + final);
          }
          
          transcriptionDiv.textContent = (final + ' ' + interim).trim() || 'Listening...';
        };
        
        recognition.onerror = (event) => {
          log('‚ùå Speech error: ' + event.error);
          statusDiv.className = 'status error';
          statusDiv.textContent = '‚ùå Error: ' + event.error;
        };
        
        recognition.onend = () => {
          log('üõë Speech recognition ended');
        };
        
        // Start everything
        recognition.start();
        
        // Show visualizer
        canvas.style.display = 'block';
        document.getElementById('testBtn').style.display = 'none';
        document.getElementById('stopBtn').style.display = 'inline-block';
        
        // Start visualization
        visualize();
        
      } catch (error) {
        log('‚ùå ERROR: ' + error.message);
        statusDiv.className = 'status error';
        statusDiv.textContent = '‚ùå ' + error.message;
      }
    }

    function visualize() {
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      
      function draw() {
        animationId = requestAnimationFrame(draw);
        
        analyser.getByteFrequencyData(dataArray);
        
        ctx.fillStyle = '#f5f5f5';
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        
        const barWidth = (canvas.width / bufferLength) * 2.5;
        let x = 0;
        
        for (let i = 0; i < bufferLength; i++) {
          const barHeight = (dataArray[i] / 255) * canvas.height * 0.8;
          const hue = (i / bufferLength) * 60 + 120;
          ctx.fillStyle = `hsl(${hue}, 70%, 50%)`;
          ctx.fillRect(x, canvas.height / 2 - barHeight / 2, barWidth, barHeight);
          x += barWidth + 1;
        }
      }
      
      draw();
    }

    function stopTest() {
      if (recognition) {
        recognition.stop();
      }
      if (audioContext) {
        audioContext.close();
      }
      if (animationId) {
        cancelAnimationFrame(animationId);
      }
      
      canvas.style.display = 'none';
      document.getElementById('testBtn').style.display = 'inline-block';
      document.getElementById('stopBtn').style.display = 'none';
      statusDiv.className = 'status info';
      statusDiv.textContent = 'Test stopped. Click "Test Microphone" to try again.';
      log('üõë Test stopped');
    }
  </script>
</body>
</html>